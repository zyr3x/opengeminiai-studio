<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini to OpenAI Proxy</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; padding: 2em; max-width: 800px; margin: auto; color: #333; background-color: #f9f9f9; }
        h1, h2 { color: #1a73e8; }
        code { background-color: #e0e0e0; padding: 2px 6px; border-radius: 4px; font-family: "SF Mono", "Fira Code", "Source Code Pro", monospace; }
        pre { background-color: #e0e0e0; padding: 1em; border-radius: 4px; overflow-x: auto; }
        .container { background-color: #fff; padding: 2em; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }
        .footer { margin-top: 2em; text-align: center; font-size: 0.9em; color: #777; }
        li { margin-bottom: 0.5em; }
        input[type="text"] { padding: 8px; width: 70%; border-radius: 4px; border: 1px solid #ccc; }
        input[type="submit"] { padding: 8px 16px; border-radius: 4px; border: none; background-color: #1a73e8; color: white; cursor: pointer; }
        textarea { padding: 8px; border-radius: 4px; border: 1px solid #ccc; width: 95%; font-family: "SF Mono", "Fira Code", "Source Code Pro", monospace; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Gemini to OpenAI Proxy</h1>
        <p>This is a lightweight proxy server that translates requests from an OpenAI-compatible client (like JetBrains AI Assistant) to Google's Gemini API.</p>

        <h2>API Key Setup</h2>
        <p>You can set your Gemini API Key using the form below. The key is stored in memory and will be reset when the server restarts. You can also set it permanently using the <code>API_KEY</code> environment variable.</p>
        <form action="/set_api_key" method="POST">
            <label for="api_key"><b>Gemini API Key:</b></label><br>
            <input type="text" id="api_key" name="api_key" placeholder="Enter your Gemini API key" size="60" value="{{ API_KEY or '' }}">
            <input type="submit" value="Save Key">
        </form>
        <p><b>Current Status:</b> The API Key is currently <strong>{{ api_key_status }}</strong>.</p>

        <h2>MCP Tools Configuration</h2>
        <p>Configure MCP tools by providing a JSON configuration. This will be saved to <code>mcp_config.json</code>.</p>
        <p>You can also use keywords in your prompt to control tool usage: <code>--nocmds</code> to disable them for a single request.</p>
        <form action="/set_mcp_config" method="POST">
            <label for="mcp_config"><b>MCP JSON Config:</b></label><br>
            <textarea id="mcp_config" name="mcp_config" rows="15">{{ current_mcp_config_str or default_mcp_config_json }}</textarea><br><br>
            <input type="submit" value="Save MCP Config">
        </form>

        <h2>Prompt Engineering</h2>
        <p>Define different profiles for prompt overrides. Each profile has a name, a list of 'triggers' to activate it, and an 'overrides' dictionary for replacements. The proxy will check incoming messages for trigger phrases and apply the overrides from the first matching profile. This is useful for handling different contexts, like regular chat vs. commit message generation.</p>
        <form action="/set_prompt_config" method="POST">
            <label for="prompt_overrides"><b>Prompt Overrides (JSON):</b></label><br>
            <textarea id="prompt_overrides" name="prompt_overrides" rows="10">{{ current_prompt_overrides_str or default_prompt_overrides_json }}</textarea><br><br>
            <input type="submit" value="Save Prompt Overrides">
        </form>

        <h2>What It Does</h2>
        <ul>
            <li>Accepts requests on OpenAI-like endpoints: <code>/v1/chat/completions</code> and <code>/v1/models</code>.</li>
            <li>Transforms the request format from OpenAI's structure to Gemini's structure.</li>
            <li>Handles streaming responses for chat completions.</li>
            <li>Manages basic conversation history truncation to fit within the model's token limits.</li>
            <li>Caches model lists to reduce upstream API calls.</li>
            <li>Supports multimodal requests (text and images).</li>
            <li>Supports MCP tools for function calling.</li>
        </ul>

        <h2>How to Use</h2>

        <h3>1. Setup and Run</h3>
        <p>Before running the server, you need to set one environment variable (or use the form above for the API Key):</p>
        <pre><code> export API_KEY="YOUR_GEMINI_API_KEY"
 export UPSTREAM_URL="https://generativelanguage.googleapis.com"</code></pre>
        <p>Then, run the server:</p>
        <pre><code> python3.12 -m venv
 source venv/bin/activate
 pip install -r requirements.txt
 python3.12 gemini-proxy.py</code></pre>
        <p>or Docker Version</p>
        <pre><code>docker-composer up -d</code></pre>
        <p>The server will start on <code>http://0.0.0.0:8080</code> by default.</p>


        <h3>2. Configure JetBrains AI Assistant</h3>
        <p>To use this proxy with JetBrains IDEs:</p>
        <ol>
            <li>Open AI Assistant settings (<code>Settings</code> > <code>Tools</code> > <code>AI Assistant</code> > <code>Models</code>).</li>
            <li>Select the "OpenAI API" service.</li>
            <li>Set the <b>Server URL</b> to: <code>http://&lt;your-server-ip-or-localhost&gt;:8080/v1/</code></li>
            <li>The model list will be fetched automatically. You can leave it as default or choose a specific one.</li>
        </ol>
        <p><b>Note:</b> The path must end with <code>/v1/</code> because the IDE will append <code>/chat/completions</code> or <code>/models</code> to it.</p>

        <h2>Available Endpoints</h2>
        <ul>
            <li><code>GET /</code>: This documentation and setup page.</li>
            <li><code>GET /v1/models</code>: Lists available Gemini models in OpenAI format.</li>
            <li><code>POST /v1/chat/completions</code>: The main endpoint for chat completions. Supports streaming.</li>
        </ul>

        <div class="footer">
            <p>Proxy server is running and ready to serve requests.</p>
        </div>
    </div>
</body>
</html>
