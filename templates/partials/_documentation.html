<!-- Documentation Section -->
<div id="documentation" class="content-section">
    <h1 class="my-4">üìö Documentation</h1>
    <p class="lead">Dive into the details of OpenGeminiAI Studio v2.0: learn how to set up, configure, and maximize the utility of this powerful proxy for Google's Gemini API.</p>

    <h2 class="mt-4">‚ú® What It Does</h2>
    <ul>
        <li><b>OpenAI API Compatibility:</b> Accepts requests on <code>/v1/chat/completions</code> and <code>/v1/models</code>, transforming them for the Gemini API.</li>
        <li><b>Streaming & History:</b> Handles streaming responses and automatically truncates conversation history to fit context windows.</li>
        <li><b>Multimodal & Code Injection:</b> Supports image, audio, and PDF file embedding directly from local paths (e.g., <code>image_path=...</code>). It also features a powerful <b>code injector</b> (<code>code_path=...</code>) that can embed single files or entire directory structures as text context in your prompt.</li>
        <li><b>Function Calling:</b> Integrates with external tools via the Multi-Tool Communication Protocol (MCP), with context-aware tool selection to improve accuracy. It also supports enabling native Google tools like Google Search via prompt profiles.</li>
        <li><b>Image Generation:</b> Includes a dedicated UI and API endpoint for generating images with compatible models.</li>
        <li><b>Advanced Prompt Control:</b> Lets you define and apply both system-level prompts and dynamic prompt overrides based on keywords.</li>
        <li><b>Multi-Chat UI:</b> The web interface supports multiple, persistent chat sessions, each with its own history and file storage.</li>
        <li><b>Efficient Caching:</b> Caches model lists and model-specific details (like token limits) to reduce upstream API calls.</li>
        <li><b>Integrated Developer Toolkit:</b> The Docker container includes a ready-to-use development environment with **Node.js 22+** and the **Docker CLI**. This enables direct execution of `npx mcp-tools` for testing function calls and allows scripts inside the container to manage the host's Docker environment via the mounted Docker socket.</li>
    </ul>

    <h2 class="mt-4">üåê Web Interface</h2>
    <p>The proxy includes a comprehensive web interface at the root URL (<code>/</code>) for configuration and testing.</p>
    <ul>
        <li><strong>Chat:</strong> An advanced interface for testing models. Features include multi-chat management, conversation history, file uploads, a dedicated image generation mode, system prompts, and manual tool selection for function calls.</li>
        <li><strong>Configuration:</strong> Set your Gemini API Key and Upstream URL. Changes are saved to the <code>.env</code> file.</li>
        <li><strong>Prompts:</strong> Create, edit, and manage libraries of reusable prompt overrides and system prompts.</li>
        <li><strong>MCP:</strong> Configure MCP (Multi-Tool Communication Protocol) tools for function calling and test them.</li>
        <li><strong>Documentation:</strong> View API endpoint details and setup instructions.</li>
    </ul>
    <p><em>Screenshot of the Web Interface:</em></p>
    <img src="/static/img/placeholder_web_ui.png" class="img-fluid rounded shadow-sm" alt="OpenGeminiAI Studio Web Interface">

    <h2 class="mt-4">üöÄ How to Use</h2>

    <h3>1. Setup and Run</h3>
    <p>Before running the server, you need to set one environment variable (or use the form above for the API Key):</p>
    <pre><code class="language-bash">
 export API_KEY="YOUR_GEMINI_API_KEY"
 export UPSTREAM_URL="https://generativelanguage.googleapis.com"
 export SERVER_HOST=0.0.0.0
 export SERVER_PORT=8080
    </code></pre>
    <p>Then, run the server:</p>
    <pre><code class="language-bash">
 python3.12 -m venv
 source venv/bin/activate
 pip install -r requirements.txt
 python3.12 run.py
    </code></pre>
    <p>or Docker Version</p>
    <pre><code class="language-bash">docker-composer up -d</code></pre>
    <p>The server will start on <code>http://0.0.0.0:8080</code> by default.</p>


    <h3 class="mt-4">2. Configure JetBrains AI Assistant</h3>
    <p>To use this proxy with JetBrains IDEs:</p>
    <ol>
        <li>Open AI Assistant settings (<code>Settings</code> > <code>Tools</code> > <code>AI Assistant</code> > <code>Models</code>).</li>
        <li>Select the "OpenAI API" service.</li>
        <li>Set the <b>Server URL</b> to: <code>http://&lt;your-server-ip-or-localhost&gt;:8080/v1/</code></li>
        <li>The model list will be fetched automatically. You can leave it as default or choose a specific one.</li>
    </ol>
    <p class="alert alert-info"><b>Note:</b> The path must end with <code>/v1/</code> because the IDE will append <code>/chat/completions</code> or <code>/models</code> to it.</p>
    <p class="mt-3"><em>Screenshot of JetBrains AI Assistant settings:</em></p>
    <img src="/static/img/placeholder_jetbrains_config.png" class="img-fluid rounded shadow-sm" alt="JetBrains AI Assistant Configuration">

    <h3 class="mt-4">3. Local File Embedding & Code Injection (via Proxy)</h3>
    <p>The proxy can automatically embed local files into your request directly from the prompt. This is especially useful when using the proxy with tools like JetBrains AI Assistant, as it allows you to reference local project files without leaving your IDE.</p>

    <h4>Multimodal Files (image, pdf, audio)</h4>
    <p>Specify the path to the file using <code>image_path=</code>, <code>pdf_path=</code>, or <code>audio_path=</code>. The proxy will read the file, encode it, and include it as a multimodal part in the request to the Gemini API.</p>
    <pre><code class="language-bash">
# Example prompts sent to the proxy:
"Please describe this image: image_path=/Users/me/Pictures/diagram.png"
"Summarize this document: pdf_path=~/Documents/report.pdf"
    </code></pre>

    <h4 class="mt-4">Code Injection (code_path)</h4>
    <p>Use <code>code_path=</code> to inject source code as plain text context. This is a powerful feature for code analysis, refactoring, or documentation tasks.</p>
    <ul>
        <li><b>Single File:</b> Provide a path to a single file (e.g., <code>code_path=app/main.py</code>).</li>
        <li><b>Entire Directory:</b> Provide a path to a directory (e.g., <code>code_path=app/controllers/</code>). The proxy will recursively read all non-hidden files, format them, and inject them into the prompt.</li>
    </ul>
    <pre><code class="language-bash">
# Example prompt for code analysis:
"Analyze the following code for potential bugs: code_path=/path/to/my_project/src"

# Example with custom ignores:
"Refactor this code: code_path=./ ignore_dir=tests ignore_file=*.md"
    </code></pre>
    <div class="alert alert-info">
        <h5 class="alert-heading">How Code Injection Works</h5>
        <p>
            <b>Intelligent Filtering:</b> The directory scanner is optimized for code analysis and automatically ignores common version control folders (<code>.git</code>), dependency folders (<code>node_modules</code>), build artifacts (<code>dist</code>, <code>target</code>), compiled files (<code>*.pyc</code>), archives, images, and other non-source files to provide clean, relevant context.
        </p>
        <p>
            <b>Custom Ignores:</b> You can add your own ignore patterns directly in the prompt. Multiple patterns can be separated by <code>|</code>.
            <ul>
                <li><code>ignore_file=PATTERN</code>: Ignores files matching the pattern (e.g., <code>*.log|*.tmp</code>).</li>
                <li><code>ignore_dir=PATTERN</code>: Ignores directories matching the pattern (e.g., <code>docs|build</code>).</li>
                <li><code>ignore_type=EXT</code>: A shorthand to ignore files by extension (e.g., <code>md|txt</code> becomes patterns <code>*.md</code> and <code>*.txt</code>).</li>
            </ul>
        </p>
        <hr>
        <p class="mb-0">
            <b>Size Limits:</b> For performance and to respect context window limits, code injection operates in "TEXT MODE" up to a configurable size limit (default 512 KB). The total size of all files to be injected must not exceed a binary limit of 8MB. If limits are exceeded, the proxy will inform you in the response instead of attaching the files.
        </p>
    </div>

    <h2 class="mt-4">üì° Available Endpoints</h2>
    <ul>
        <li><code>GET /</code>: This documentation and setup page.</li>
        <li><code>GET /v1/models</code>: Lists available Gemini models in OpenAI format.</li>
        <li><code>POST /v1/chat/completions</code>: The main endpoint for chat completions. Supports streaming.</li>
    </ul>
</div>
