<!-- Documentation Section -->
<div id="documentation" class="content-section">
    <h1 class="my-4">üìö Documentation</h1>
    <p class="lead">Dive into the details of OpenGeminiAI Studio v1.1: learn how to set up, configure, and maximize the utility of this powerful proxy for Google's Gemini API.</p>

    <h2 class="mt-4">‚ú® What It Does</h2>
    <ul>
        <li><b>OpenAI API Compatibility:</b> Accepts requests on <code>/v1/chat/completions</code> and <code>/v1/models</code>, transforming them for the Gemini API.</li>
        <li><b>Streaming & History:</b> Handles streaming responses and automatically truncates conversation history to fit context windows.</li>
        <li><b>Multimodal Support:</b> Supports image and text inputs. It can even automatically embed local files from your prompts when you use syntax like <code>image_path=/path/to/file.png</code> or <code>pdf_path=/path/to/file.pdf</code>.</li>
        <li><b>Function Calling:</b> Integrates with external tools via the Multi-Tool Communication Protocol (MCP), with context-aware tool selection to improve accuracy.</li>
        <li><b>Image Generation:</b> Includes a dedicated UI and API endpoint for generating images with compatible models.</li>
        <li><b>Advanced Prompt Control:</b> Lets you define and apply both system-level prompts and dynamic prompt overrides based on keywords.</li>
        <li><b>Multi-Chat UI:</b> The web interface supports multiple, persistent chat sessions, each with its own history and file storage.</li>
        <li><b>Efficient Caching:</b> Caches model lists to reduce upstream API calls.</li>
    </ul>

    <h2 class="mt-4">üåê Web Interface</h2>
    <p>The proxy includes a comprehensive web interface at the root URL (<code>/</code>) for configuration and testing.</p>
    <ul>
        <li><strong>Chat:</strong> An advanced interface for testing models. Features include multi-chat management, conversation history, file uploads, a dedicated image generation mode, system prompts, and manual tool selection for function calls.</li>
        <li><strong>Configuration:</strong> Set your Gemini API Key and Upstream URL. Changes are saved to the <code>.env</code> file.</li>
        <li><strong>Prompts:</strong> Create, edit, and manage libraries of reusable prompt overrides and system prompts.</li>
        <li><strong>MCP:</strong> Configure MCP (Multi-Tool Communication Protocol) tools for function calling and test them.</li>
        <li><strong>Documentation:</strong> View API endpoint details and setup instructions.</li>
    </ul>
    <p><em>Screenshot of the Web Interface:</em></p>
    <img src="/static/img/placeholder_web_ui.png" class="img-fluid rounded shadow-sm" alt="OpenGeminiAI Studio Web Interface">

    <h2 class="mt-4">üöÄ How to Use</h2>

    <h3>1. Setup and Run</h3>
    <p>Before running the server, you need to set one environment variable (or use the form above for the API Key):</p>
    <pre><code class="language-bash">
 export API_KEY="YOUR_GEMINI_API_KEY"
 export UPSTREAM_URL="https://generativelanguage.googleapis.com"
 export SERVER_HOST=0.0.0.0
 export SERVER_PORT=8080
    </code></pre>
    <p>Then, run the server:</p>
    <pre><code class="language-bash">
 python3.12 -m venv
 source venv/bin/activate
 pip install -r requirements.txt
 python3.12 run.py
    </code></pre>
    <p>or Docker Version</p>
    <pre><code class="language-bash">docker-composer up -d</code></pre>
    <p>The server will start on <code>http://0.0.0.0:8080</code> by default.</p>


    <h3 class="mt-4">2. Configure JetBrains AI Assistant</h3>
    <p>To use this proxy with JetBrains IDEs:</p>
    <ol>
        <li>Open AI Assistant settings (<code>Settings</code> > <code>Tools</code> > <code>AI Assistant</code> > <code>Models</code>).</li>
        <li>Select the "OpenAI API" service.</li>
        <li>Set the <b>Server URL</b> to: <code>http://&lt;your-server-ip-or-localhost&gt;:8080/v1/</code></li>
        <li>The model list will be fetched automatically. You can leave it as default or choose a specific one.</li>
    </ol>
    <p class="alert alert-info"><b>Note:</b> The path must end with <code>/v1/</code> because the IDE will append <code>/chat/completions</code> or <code>/models</code> to it.</p>
    <p class="mt-3"><em>Screenshot of JetBrains AI Assistant settings:</em></p>
    <img src="/static/img/placeholder_jetbrains_config.png" class="img-fluid rounded shadow-sm" alt="JetBrains AI Assistant Configuration">

    <h3 class="mt-4">3. Local File Embedding (via Proxy)</h3>
    <p>The proxy can automatically embed local image or PDF files into your request. In your prompt, simply specify the path to the file using <code>image_path=</code> or <code>pdf_path=</code>.</p>
    <pre><code class="language-bash">
# Example prompt sent to the proxy:
"Please describe this image: image_path=/Users/me/Pictures/diagram.png"
    </code></pre>
    <p>The proxy will read the file, encode it, and include it in the request to the Gemini API as multimodal content. This is especially useful when using the proxy with tools like JetBrains AI Assistant.</p>

    <h2 class="mt-4">üì° Available Endpoints</h2>
    <ul>
        <li><code>GET /</code>: This documentation and setup page.</li>
        <li><code>GET /v1/models</code>: Lists available Gemini models in OpenAI format.</li>
        <li><code>POST /v1/chat/completions</code>: The main endpoint for chat completions. Supports streaming.</li>
    </ul>
</div>
