<!-- Documentation Section -->
<div id="documentation" class="content-section">
    <h1 class="my-4">üìö Documentation</h1>
    <p class="lead">Dive into the details of OpenGeminiAI Studio: learn how to set up, configure, and maximize the utility of this powerful proxy for Google's Gemini API.</p>

    <h2 class="mt-4">‚ú® What It Does</h2>
    <ul>
        <li>Accepts requests on OpenAI-like endpoints: <code>/v1/chat/completions</code> and <code>/v1/models</code>.</li>
        <li>Transforms the request format from OpenAI's structure to Gemini's structure.</li>
        <li>Handles streaming responses for chat completions.</li>
        <li>Manages basic conversation history truncation to fit within the model's token limits.</li>
        <li>Caches model lists to reduce upstream API calls.</li>
        <li>Supports multimodal requests (text and images).</li>
        <li>Supports MCP tools for function calling.</li>
    </ul>

    <h2 class="mt-4">üåê Web Interface</h2>
    <p>The proxy includes a comprehensive web interface at the root URL (<code>/</code>) for configuration and testing.</p>
    <ul>
        <li><strong>Chat:</strong> A simple interface to test models and conversation history.</li>
        <li><strong>Configuration:</strong> Set your Gemini API Key and Upstream URL. Changes are saved to the <code>.env</code> file.</li>
        <li><strong>Prompts:</strong> Create, edit, and manage a library of reusable prompts.</li>
        <li><strong>MCP:</strong> Configure MCP (Multi-Tool Communication Protocol) tools for function calling.</li>
        <li><strong>Documentation:</strong> View API endpoint details and setup instructions.</li>
    </ul>
    <p><em>Screenshot of the Web Interface:</em></p>
    <img src="/static/img/placeholder_web_ui.png" class="img-fluid rounded" alt="OpenGeminiAI Studio Web Interface">

    <h2 class="mt-4">üöÄ How to Use</h2>

    <h3>1. Setup and Run</h3>
    <p>Before running the server, you need to set one environment variable (or use the form above for the API Key):</p>
    <pre><code class="language-bash">
 export API_KEY="YOUR_GEMINI_API_KEY"
 export UPSTREAM_URL="https://generativelanguage.googleapis.com"
 export SERVER_HOST=0.0.0.0
 export SERVER_PORT=8080
    </code></pre>
    <p>Then, run the server:</p>
    <pre><code class="language-bash">
 python3.12 -m venv
 source venv/bin/activate
 pip install -r requirements.txt
 python3.12 run.py
    </code></pre>
    <p>or Docker Version</p>
    <pre><code class="language-bash">docker-composer up -d</code></pre>
    <p>The server will start on <code>http://0.0.0.0:8080</code> by default.</p>


    <h3 class="mt-4">2. Configure JetBrains AI Assistant</h3>
    <p>To use this proxy with JetBrains IDEs:</p>
    <ol>
        <li>Open AI Assistant settings (<code>Settings</code> > <code>Tools</code> > <code>AI Assistant</code> > <code>Models</code>).</li>
        <li>Select the "OpenAI API" service.</li>
        <li>Set the <b>Server URL</b> to: <code>http://&lt;your-server-ip-or-localhost&gt;:8080/v1/</code></li>
        <li>The model list will be fetched automatically. You can leave it as default or choose a specific one.</li>
    </ol>
    <p class="alert alert-info"><b>Note:</b> The path must end with <code>/v1/</code> because the IDE will append <code>/chat/completions</code> or <code>/models</code> to it.</p>
    <p class="mt-3"><em>Screenshot of JetBrains AI Assistant settings:</em></p>
    <img src="/static/img/placeholder_jetbrains_config.png" class="img-fluid rounded" alt="JetBrains AI Assistant Configuration">

    <h2 class="mt-4">üì° Available Endpoints</h2>
    <ul>
        <li><code>GET /</code>: This documentation and setup page.</li>
        <li><code>GET /v1/models</code>: Lists available Gemini models in OpenAI format.</li>
        <li><code>POST /v1/chat/completions</code>: The main endpoint for chat completions. Supports streaming.</li>
    </ul>
</div>
